{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter path of input image : \n",
      "ocr/test4.png\n",
      "\n",
      "OCR in progress...\n",
      "Input image scanned and saved in :  out_text.txt\n",
      "\n",
      "Applying spell check and correction...\n",
      "\n",
      "Spell check done and changes made and saved in :  after_spell_text.txt\n",
      "\n",
      "Choose from the following : \n",
      "1. Fill in the Blanks \n",
      "2. Multiple Choice Questions \n",
      "3. True or False Questions\n",
      "1\n",
      "\n",
      "Fill in the Blanks Question Generation Starting...\n",
      "Questions Generated and saved in : generatedQuestions.txt\n"
     ]
    }
   ],
   "source": [
    "print ('Enter path of input image : ')\n",
    "img_path = input()\n",
    "#img_path = 'ocr/test5.png'\n",
    "print ('\\nOCR in progress...')\n",
    "ocrTextPath = OCR(img_path)\n",
    "print (\"Input image scanned and saved in : \", ocrTextPath)\n",
    "print ('\\nApplying spell check and correction...')\n",
    "afterSpellCheckPath  = applySpellCheck(ocrTextPath)\n",
    "print ('\\nSpell check done and changes made and saved in : ', afterSpellCheckPath)\n",
    "startgeneration(afterSpellCheckPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startgeneration(afterSpellCheckPath):\n",
    "    print ('\\nChoose from the following : \\n1. Fill in the Blanks \\n2. Multiple Choice Questions \\n3. True or False Questions')\n",
    "\n",
    "    file = open (afterSpellCheckPath , 'r')\n",
    "    lines = file.readlines()\n",
    "    text = ''\n",
    "\n",
    "    for i in lines:\n",
    "        text = text + i\n",
    "\n",
    "    text = ' '.join(text.split(\"\\n\"))\n",
    "    option = int (input())\n",
    "    \n",
    "    flag = 0\n",
    "    \n",
    "    if (option == 1):\n",
    "        print ('\\nFill in the Blanks Question Generation Starting...')\n",
    "        flag = 1\n",
    "    elif (option == 2):\n",
    "        print ('\\nMultiple Choice Question Generation Starting...')   \n",
    "        flag = 1\n",
    "    elif (option == 3):\n",
    "        print ('\\nTrue or False Question Generation Starting...')\n",
    "        flag = 1\n",
    "    else:\n",
    "        print ('\\nInvalid Input')\n",
    "        flag = 0\n",
    "    \n",
    "    if (flag == 1):\n",
    "        generateQuestions(text, option)\n",
    "        print ('Questions Generated and saved in : generatedQuestions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "from pathlib import Path\n",
    "\n",
    "def dumpPickle(fileName, content):\n",
    "    pickleFile = open(fileName, 'wb')\n",
    "    cPickle.dump(content, pickleFile, -1)\n",
    "    pickleFile.close()\n",
    "\n",
    "def loadPickle(fileName):    \n",
    "    file = open(fileName, 'rb')\n",
    "    content = cPickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return content\n",
    "    \n",
    "def pickleExists(fileName):\n",
    "    file = Path(fileName)\n",
    "    \n",
    "    if file.is_file():\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "def OCR(path):\n",
    "    outfile = \"out_text.txt\"\n",
    "    f = open(outfile, \"w\")\n",
    "    text = str(pytesseract.image_to_string(Image.open(path)))\n",
    "    text = text.replace('-\\n', '')\n",
    "    f.write(text)\n",
    "    f.close()\n",
    "    return (outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker \n",
    "import nltk\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_properNoun(line,properNoun):\n",
    "    tagged_words=nltk.tag.pos_tag(line.split())\n",
    "    for word in tagged_words:\n",
    "        if(word[1]=='NNP'):     #'NNP' -> Tag for Proper Noun\n",
    "            properNoun.append(word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converttostr(input_seq, seperator):\n",
    "   # Join all the strings in list\n",
    "   final_str = seperator.join(input_seq)\n",
    "   return final_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellCheck(line):\n",
    "    sep = \"\"\n",
    "    corrected_word = \"\"\n",
    "    corrected_list = list()\n",
    "    corrected_line = \"\"\n",
    "    \n",
    "    spell = SpellChecker() \n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        if(word[-1].isalnum()==False):\n",
    "            sep = word[-1]\n",
    "            corrected_word = spell.correction(word[0:-1])\n",
    "            corrected_word+=(sep)\n",
    "        else:\n",
    "            corrected_word = spell.correction(word)\n",
    "            \n",
    "        corrected_list.append(corrected_word)  #Append every corrected word to corrected list\n",
    "        \n",
    "    to_join = ' '  #Merge the corrected list to a line\n",
    "    corrected_line += converttostr(corrected_list, to_join)  \n",
    "    \n",
    "    return corrected_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySpellCheck(path):\n",
    "    outfile = 'after_spell_text.txt'\n",
    "    file2 = open(outfile, 'w')\n",
    "    file1 = open('out_text.txt', 'r')\n",
    "    properNoun=list()               #list of proper nouns\n",
    "\n",
    "    Lines = file1.readlines()\n",
    "    for line in Lines:\n",
    "        append_properNoun(line,properNoun)            #Identifies proper nouns\n",
    "        spell.word_frequency.load_words(properNoun)   #Includes the proper nouns in 'known' words\n",
    "        new_line=spellCheck(line)                     #Rectifies the mispelled words \n",
    "        file2.write(new_line)                         #Appends corrected line to another .txt file\n",
    "        file2.write(\"\\n\")\n",
    "    file1.close()\n",
    "    file2.close()\n",
    "    return (outfile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Extract all words from plain text and generate it's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#Extract answers and the sentence they are in\n",
    "def extractAnswers(qas, doc):\n",
    "    answers = []\n",
    "\n",
    "    senStart = 0\n",
    "    senId = 0\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        senLen = len(sentence.text)\n",
    "\n",
    "        for answer in qas:\n",
    "            answerStart = answer['answers'][0]['answer_start']\n",
    "\n",
    "            if (answerStart >= senStart and answerStart < (senStart + senLen)):\n",
    "                answers.append({'sentenceId': senId, 'text': answer['answers'][0]['text']})\n",
    "\n",
    "        senStart += senLen\n",
    "        senId += 1\n",
    "    \n",
    "    return answers\n",
    "\n",
    "#TODO - Clean answers from stopwords?\n",
    "def tokenIsAnswer(token, sentenceId, answers):\n",
    "    for i in range(len(answers)):\n",
    "        if (answers[i]['sentenceId'] == sentenceId):\n",
    "            if (answers[i]['text'] == token):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "#Save named entities start points\n",
    "\n",
    "def getNEStartIndexs(doc):\n",
    "    neStarts = {}\n",
    "    for ne in doc.ents:\n",
    "        neStarts[ne.start] = ne\n",
    "        \n",
    "    return neStarts \n",
    "\n",
    "def getSentenceStartIndexes(doc):\n",
    "    senStarts = []\n",
    "    \n",
    "    for sentence in doc.sents:\n",
    "        senStarts.append(sentence[0].i)\n",
    "    \n",
    "    return senStarts\n",
    "    \n",
    "def getSentenceForWordPosition(wordPos, senStarts):\n",
    "    for i in range(1, len(senStarts)):\n",
    "        if (wordPos < senStarts[i]):\n",
    "            return i - 1\n",
    "        \n",
    "def addWordsForParagrapgh(newWords, text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    neStarts = getNEStartIndexs(doc)\n",
    "    senStarts = getSentenceStartIndexes(doc)\n",
    "    \n",
    "    #index of word in spacy doc text\n",
    "    i = 0\n",
    "    \n",
    "    while (i < len(doc)):\n",
    "        #If the token is a start of a Named Entity, add it and push to index to end of the NE\n",
    "        if (i in neStarts):\n",
    "            word = neStarts[i]\n",
    "            #add word\n",
    "            currentSentence = getSentenceForWordPosition(word.start, senStarts)\n",
    "            wordLen = word.end - word.start\n",
    "            shape = ''\n",
    "            for wordIndex in range(word.start, word.end):\n",
    "                shape += (' ' + doc[wordIndex].shape_)\n",
    "\n",
    "            newWords.append([word.text,\n",
    "                            0,\n",
    "                            0,\n",
    "                            currentSentence,\n",
    "                            wordLen,\n",
    "                            word.label_,\n",
    "                            None,\n",
    "                            None,\n",
    "                            None,\n",
    "                            shape])\n",
    "            i = neStarts[i].end - 1\n",
    "        #If not a NE, add the word if it's not a stopword or a non-alpha (not regular letters)\n",
    "        else:\n",
    "            if (doc[i].is_stop == False and doc[i].is_alpha == True):\n",
    "                word = doc[i]\n",
    "\n",
    "                currentSentence = getSentenceForWordPosition(i, senStarts)\n",
    "                wordLen = 1\n",
    "\n",
    "                newWords.append([word.text,\n",
    "                                0,\n",
    "                                0,\n",
    "                                currentSentence,\n",
    "                                wordLen,\n",
    "                                None,\n",
    "                                word.pos_,\n",
    "                                word.tag_,\n",
    "                                word.dep_,\n",
    "                                word.shape_])\n",
    "        i += 1\n",
    "\n",
    "def oneHotEncodeColumns(df):\n",
    "    columnsToEncode = ['NER', 'POS', \"TAG\", 'DEP']\n",
    "\n",
    "    for column in columnsToEncode:\n",
    "        one_hot = pd.get_dummies(df[column])\n",
    "        one_hot = one_hot.add_prefix(column + '_')\n",
    "\n",
    "        df = df.drop(column, axis = 1)\n",
    "        df = df.join(one_hot)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Predict whether a word is a keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDf(text):\n",
    "    words = []\n",
    "    addWordsForParagrapgh(words, text)\n",
    "\n",
    "    wordColums = ['text', 'titleId', 'paragrapghId', 'sentenceId','wordCount', 'NER', 'POS', 'TAG', 'DEP','shape']\n",
    "    df = pd.DataFrame(words, columns=wordColums)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDf(df):\n",
    "    #One-hot encoding\n",
    "    wordsDf = oneHotEncodeColumns(df)\n",
    "\n",
    "    #Drop unused columns\n",
    "    columnsToDrop = ['text', 'titleId', 'paragrapghId', 'sentenceId', 'shape']\n",
    "    wordsDf = wordsDf.drop(columnsToDrop, axis = 1)\n",
    "\n",
    "    #Add missing colums \n",
    "    predictorColumns = ['wordCount','NER_CARDINAL','NER_DATE','NER_EVENT','NER_FAC','NER_GPE','NER_LANGUAGE','NER_LAW','NER_LOC','NER_MONEY','NER_NORP','NER_ORDINAL','NER_ORG','NER_PERCENT','NER_PERSON','NER_PRODUCT','NER_QUANTITY','NER_TIME','NER_WORK_OF_ART','POS_ADJ','POS_ADP','POS_ADV','POS_CCONJ','POS_DET','POS_INTJ','POS_NOUN','POS_NUM','POS_PART','POS_PRON','POS_PROPN','POS_PUNCT','POS_SYM','POS_VERB','POS_X','TAG_''','TAG_-LRB-','TAG_.','TAG_ADD','TAG_AFX','TAG_CC','TAG_CD','TAG_DT','TAG_EX','TAG_FW','TAG_IN','TAG_JJ','TAG_JJR','TAG_JJS','TAG_LS','TAG_MD','TAG_NFP','TAG_NN','TAG_NNP','TAG_NNPS','TAG_NNS','TAG_PDT','TAG_POS','TAG_PRP','TAG_PRP$','TAG_RB','TAG_RBR','TAG_RBS','TAG_RP','TAG_SYM','TAG_TO','TAG_UH','TAG_VB','TAG_VBD','TAG_VBG','TAG_VBN','TAG_VBP','TAG_VBZ','TAG_WDT','TAG_WP','TAG_WRB','TAG_XX','DEP_ROOT','DEP_acl','DEP_acomp','DEP_advcl','DEP_advmod','DEP_agent','DEP_amod','DEP_appos','DEP_attr','DEP_aux','DEP_auxpass','DEP_case','DEP_cc','DEP_ccomp','DEP_compound','DEP_conj','DEP_csubj','DEP_csubjpass','DEP_dative','DEP_dep','DEP_det','DEP_dobj','DEP_expl','DEP_intj','DEP_mark','DEP_meta','DEP_neg','DEP_nmod','DEP_npadvmod','DEP_nsubj','DEP_nsubjpass','DEP_nummod','DEP_oprd','DEP_parataxis','DEP_pcomp','DEP_pobj','DEP_poss','DEP_preconj','DEP_predet','DEP_prep','DEP_prt','DEP_punct','DEP_quantmod','DEP_relcl','DEP_xcomp']\n",
    "\n",
    "    for feature in predictorColumns:\n",
    "        if feature not in wordsDf.columns:\n",
    "            wordsDf[feature] = 0\n",
    "    \n",
    "    return wordsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictWords(wordsDf, df):\n",
    "    \n",
    "    predictorPickleName = 'data/nb-predictor.pkl'\n",
    "    predictor = loadPickle(predictorPickleName)\n",
    "    \n",
    "    y_pred = predictor.predict_proba(wordsDf)\n",
    "\n",
    "    labeledAnswers = []\n",
    "    for i in range(len(y_pred)):\n",
    "        labeledAnswers.append({'word': df.iloc[i]['text'], 'prob': y_pred[i][0]})\n",
    "    \n",
    "    return labeledAnswers"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Extract questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blankAnswer(firstTokenIndex, lastTokenIndex, sentStart, sentEnd, doc):\n",
    "    leftPartStart = doc[sentStart].idx\n",
    "    leftPartEnd = doc[firstTokenIndex].idx\n",
    "    rightPartStart = doc[lastTokenIndex].idx + len(doc[lastTokenIndex])\n",
    "    rightPartEnd = doc[sentEnd - 1].idx + len(doc[sentEnd - 1])\n",
    "    \n",
    "    question = doc.text[leftPartStart:leftPartEnd] + '_____' + doc.text[rightPartStart:rightPartEnd]\n",
    "    \n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addQuestions(answers, text):\n",
    "    doc = nlp(text)\n",
    "    currAnswerIndex = 0\n",
    "    qaPair = []\n",
    "\n",
    "    #Check wheter each token is the next answer\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            \n",
    "            #If all the answers have been found, stop looking\n",
    "            if currAnswerIndex >= len(answers):\n",
    "                break\n",
    "            \n",
    "            #In the case where the answer is consisted of more than one token, check the following tokens as well.\n",
    "            answerDoc = nlp(answers[currAnswerIndex]['word'])\n",
    "            answerIsFound = True\n",
    "            \n",
    "            for j in range(len(answerDoc)):\n",
    "                if token.i + j >= len(doc) or doc[token.i + j].text != answerDoc[j].text:\n",
    "                    answerIsFound = False\n",
    "           \n",
    "            #If the current token is corresponding with the answer, add it \n",
    "            if answerIsFound:\n",
    "                question = blankAnswer(token.i, token.i + len(answerDoc) - 1, sent.start, sent.end, doc)\n",
    "                \n",
    "                qaPair.append({'question' : question, 'answer': answers[currAnswerIndex]['word'], 'prob': answers[currAnswerIndex]['prob']})\n",
    "                \n",
    "                currAnswerIndex += 1\n",
    "                \n",
    "    return qaPair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortAnswers(qaPairs):\n",
    "    orderedQaPairs = sorted(qaPairs, key=lambda qaPair: qaPair['prob'])\n",
    "    \n",
    "    return orderedQaPairs    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Distractors (Incorrect Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/distractor.pkl'\n",
    "model = loadPickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distractors(answer, count):\n",
    "    answer = str.lower(answer)\n",
    "    \n",
    "    ##Extracting closest words for the answer. \n",
    "    try:\n",
    "        closestWords = model.most_similar(positive=[answer], topn=count)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    distractors = list(map(lambda x: x[0], closestWords))[0:count]\n",
    "    \n",
    "    return distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addDistractors(qaPairs, count):\n",
    "    for qaPair in qaPairs:\n",
    "        distractors = generate_distractors(qaPair['answer'], count)\n",
    "        qaPair['distractors'] = distractors\n",
    "    \n",
    "    return qaPairs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Question Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateQuestions(text, option):\n",
    "    \n",
    "    # Extract words \n",
    "    df = generateDf(text)\n",
    "    wordsDf = prepareDf(df)\n",
    "    \n",
    "    # Predict \n",
    "    labeledAnswers = predictWords(wordsDf, df)\n",
    "    \n",
    "    # Transform questions\n",
    "    qaPairs = addQuestions(labeledAnswers, text)\n",
    "    \n",
    "    # Pick the best questions\n",
    "    orderedQaPairs = sortAnswers(qaPairs)\n",
    "    \n",
    "    # Generate distractors\n",
    "    questions = addDistractors(orderedQaPairs[:len(orderedQaPairs)], 3)\n",
    "\n",
    "    # Formating the output into a txt file\n",
    "    path = 'generatedQuestions.txt'\n",
    "    file = open(path, 'w')\n",
    "    \n",
    "    if (option == 1):\n",
    "        for i in range(len(orderedQaPairs)): \n",
    "            temp = 'Question ' + str(i+1) + '. '\n",
    "            file.write(temp)\n",
    "            file.write(questions[i]['question'])\n",
    "            file.write('\\n')\n",
    "            file.write('Answer : ')\n",
    "            file.write(questions[i]['answer'])\n",
    "            file.write('\\n\\n')     \n",
    "    \n",
    "    if (option == 2):\n",
    "        choice = ['a. ', 'b. ', 'c. ', 'd. ']\n",
    "        for i in range(len(orderedQaPairs)):\n",
    "            temp = 'Question ' + str(i+1) + '. ' \n",
    "            file.write(temp)\n",
    "            file.write(questions[i]['question'])\n",
    "            file.write('\\n')\n",
    "            options = []\n",
    "            a = (questions[i]['answer']).lower()\n",
    "            options.append(a)\n",
    "\n",
    "            for distractor in questions[i]['distractors']:\n",
    "                options.append(distractor)\n",
    "                \n",
    "            random.shuffle(options)\n",
    "            for j in range (len(options)):\n",
    "                temp2 = choice[j] + options[j]\n",
    "                file.write(temp2)\n",
    "                file.write('\\n') \n",
    "            temp3 = 'Answer : ' + a\n",
    "            file.write(temp3)\n",
    "            file.write('\\n\\n')\n",
    "            \n",
    "    if (option == 3):\n",
    "        for i in range(len(orderedQaPairs)):\n",
    "            temp = 'Question ' + str(i+1) + '. ' \n",
    "            file.write(temp)\n",
    "            options = []\n",
    "            options.append(questions[i]['answer'])\n",
    "\n",
    "            for distractor in questions[i]['distractors']:\n",
    "                options.append(distractor)\n",
    "\n",
    "            random.shuffle(options)\n",
    "            inputOption = random.choice(options) \n",
    "\n",
    "            file.write(questions[i]['question'].replace('_____', inputOption))\n",
    "            file.write('\\n')\n",
    "            file.write('Answer : ')\n",
    "\n",
    "            if (inputOption == questions[i]['answer']):\n",
    "                file.write ('True')\n",
    "\n",
    "            else:\n",
    "                file.write ('False')\n",
    "            \n",
    "            file.write ('\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
